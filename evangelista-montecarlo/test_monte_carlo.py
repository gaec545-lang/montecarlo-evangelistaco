from src.configuration_manager import ConfigurationManager
from src.monte_carlo_engine import UniversalMonteCarloEngine
import time
import numpy as np  

print("=" * 60)
print("TEST DE MONTE CARLO ENGINE")
print("=" * 60)

# Test 1: Cargar configuraciÃ³n
print("\n[Test 1] Cargando configuraciÃ³n...")
try:
    config = ConfigurationManager(
        template='templates/alimentos.yaml',
        client_config='clients/test_pasteleria_config.yaml'
    )
    print("âœ… ConfiguraciÃ³n cargada")
except Exception as e:
    print(f"âŒ Error: {e}")
    exit(1)

# Test 2: Inicializar engine
print("\n[Test 2] Inicializando Monte Carlo Engine...")
try:
    engine = UniversalMonteCarloEngine(config)
    print("âœ… Engine inicializado")
except Exception as e:
    print(f"âŒ Error: {e}")
    exit(1)

# Test 3: Cargar datos histÃ³ricos
print("\n[Test 3] Cargando datos histÃ³ricos...")
try:
    engine.load_historical_data()
    print("âœ… Datos histÃ³ricos procesados")
except Exception as e:
    print(f"âŒ Error: {e}")
    exit(1)

# Test 4: Setup simulaciÃ³n
print("\n[Test 4] Configurando simulaciÃ³n...")
try:
    engine.setup_simulation()
except Exception as e:
    print(f"âŒ Error: {e}")
    exit(1)

# Test 5: Ejecutar simulaciÃ³n
print("\n[Test 5] Ejecutando simulaciÃ³n...")
try:
    start = time.time()
    results = engine.run()
    elapsed = time.time() - start
    
    print(f"âœ… SimulaciÃ³n completada en {elapsed:.1f} segundos")
    print(f"   Total simulaciones: {len(results):,}")
    print(f"   Variables simuladas: {len([c for c in results.columns if c not in ['outcome', 'simulation_id']])}")
except Exception as e:
    print(f"âŒ Error: {e}")
    exit(1)

# Test 6: EstadÃ­sticas
print("\n[Test 6] Calculando estadÃ­sticas...")
try:
    stats = engine.get_statistics()
    
    print("âœ… EstadÃ­sticas calculadas:")
    print(f"\n   ğŸ“Š RESULTADOS:")
    print(f"   â”œâ”€ Media (P50): ${stats['mean']:,.0f}")
    print(f"   â”œâ”€ Mediana: ${stats['median']:,.0f}")
    print(f"   â”œâ”€ DesviaciÃ³n estÃ¡ndar: ${stats['std']:,.0f}")
    print(f"   â”œâ”€ MÃ­nimo: ${stats['min']:,.0f}")
    print(f"   â””â”€ MÃ¡ximo: ${stats['max']:,.0f}")
    
    print(f"\n   ğŸ“ˆ PERCENTILES:")
    print(f"   â”œâ”€ P10 (pesimista): ${stats['p10']:,.0f}")
    print(f"   â”œâ”€ P50 (mediana): ${stats['p50']:,.0f}")
    print(f"   â””â”€ P90 (optimista): ${stats['p90']:,.0f}")
    
    print(f"\n   âš ï¸  RIESGOS:")
    print(f"   â”œâ”€ Probabilidad de pÃ©rdida: {stats['prob_loss']:.1%}")
    print(f"   â”œâ”€ VaR 95%: ${abs(stats['var_95']):,.0f}")
    print(f"   â””â”€ CVaR 95%: ${abs(stats['cvar_95']):,.0f}")
    
except Exception as e:
    print(f"âŒ Error: {e}")
    exit(1)

# Test 7: AnÃ¡lisis de sensibilidad
print("\n[Test 7] AnÃ¡lisis de sensibilidad...")
try:
    sensitivity = engine.sensitivity_analysis()
    
    print("âœ… AnÃ¡lisis completado:")
    print("\n   ğŸ“Š IMPACTO DE VARIABLES:")
    for idx, row in sensitivity.iterrows():
        bar_length = int(row['importance'] * 50)
        bar = "â–ˆ" * bar_length
        print(f"   {row['variable']:20} {bar} {row['importance']:.1%}")
    
except Exception as e:
    print(f"âŒ Error: {e}")
    exit(1)

# Test 8: Validar resultados
print("\n[Test 8] Validando resultados...")
try:
    # Verificar que no hay NaN o infinitos
    assert not results['outcome'].isna().any(), "Hay NaN en resultados"
    assert not np.isinf(results['outcome']).any(), "Hay infinitos en resultados"
    
    # Verificar que prob_loss tiene sentido
    assert 0 <= stats['prob_loss'] <= 1, "Prob pÃ©rdida fuera de rango"
    
    # Verificar que P50 estÃ¡ entre min y max
    assert stats['min'] <= stats['p50'] <= stats['max'], "P50 fuera de rango"
    
    print("âœ… Resultados validados (sin errores lÃ³gicos)")
    
except AssertionError as e:
    print(f"âŒ {e}")
    exit(1)

print("\n" + "=" * 60)
print("ğŸ‰ TODOS LOS TESTS DE MONTE CARLO PASARON")
print("=" * 60)

# Test 8: Validar resultados lÃ³gicos
print("\n[Test 8] Validando resultados...")
try:
    assert not results['outcome'].isna().any(), "Hay NaN en resultados"
    assert not np.isinf(results['outcome']).any(), "Hay infinitos en resultados"
    assert 0 <= stats['prob_loss'] <= 1, "Prob pÃ©rdida fuera de rango"
    print("âœ… Resultados validados (sin errores lÃ³gicos)")
except AssertionError as e:
    print(f"âŒ {e}")
    exit(1)

# Test 9: Evaluar Triggers de Decision Intelligence
print("\n[Test 9] Evaluando Triggers de Negocio (Sentinel)...")
try:
    # Llamamos al nuevo mÃ©todo que construyÃ³ Claude
    triggers = engine.evaluate_triggers(stats)
    
    if triggers:
        print(f"ğŸš¨ {len(triggers)} TRIGGER(S) ACTIVADO(S):\n")
        
        # Agrupar por nivel
        criticos = [t for t in triggers if t['nivel'] == 'CRÃTICO']
        altos = [t for t in triggers if t['nivel'] == 'ALTO']
        medios = [t for t in triggers if t.get('nivel') == 'MEDIO']
        
        if criticos:
            print("   ğŸ”´ ALERTAS CRÃTICAS:")
            for trigger in criticos:
                print(f"      â€¢ {trigger['metrica']}: {trigger['mensaje']}")
        
        if altos:
            print("   ğŸŸ¡ ALERTAS ALTAS:")
            for trigger in altos:
                print(f"      â€¢ {trigger['metrica']}: {trigger['mensaje']}")
                
        if medios:
            print("   ğŸŸ  ALERTAS MEDIAS:")
            for trigger in medios:
                print(f"      â€¢ {trigger['metrica']}: {trigger['mensaje']}")
    else:
        print("   âœ… No hay alertas de riesgo.")
        print("   âœ… Todos los indicadores operativos estÃ¡n dentro de los umbrales del YAML.")
        
except AttributeError:
    print("âŒ Error: El mÃ©todo 'evaluate_triggers' no se encontrÃ³ en UniversalMonteCarloEngine.")
    print("ğŸ’¡ AsegÃºrate de haber guardado src/monte_carlo_engine.py con el nuevo cÃ³digo de la Fase 2.")
except Exception as e:
    print(f"âŒ Error inesperado al evaluar triggers: {e}")

print("\n" + "=" * 60)
print("ğŸ‰ FASE 2 COMPLETADA: MOTOR Y TRIGGERS OPERATIVOS")
print("=" * 60)
